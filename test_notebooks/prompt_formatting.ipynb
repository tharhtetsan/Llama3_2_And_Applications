{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_env\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Who wrote the book Charlotte's Web?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote the book Charlotte's Web?\"\n",
    "\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # user header\n",
    "    f\"{question}\"                                    # user input\n",
    "    \"<|eot_id|>\"                                     #end of turn\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  #assistant header\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "from  utils import llama31\n",
    "response = llama31(prompt,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"id\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"8e667ea1ee328980-SIN\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"object\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"text.completion\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"created\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1732253082\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"model\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"prompt\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m[],\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"choices\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"text\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"\\n\\nThe book \\\"Charlotte's Web\\\" was written by E.B. White.\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"finish_reason\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"eos\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"seed\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1204425200065545700\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"logprobs\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34mnull\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m            \u001b[39;49;00m\u001b[94m\"index\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[94m\"usage\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"prompt_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m17\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"completion_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m17\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"total_tokens\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m34\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "}\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import cprint\n",
    "response = llama31(prompt,8, raw=True)\n",
    "\n",
    "cprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_question = \"Three best quotes in it\"\n",
    "follow_up_prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"        # past  \n",
    "    f\"{question}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"   # past\n",
    "    f\"{response}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # new\n",
    "    f\"{follow_up_question}\"                          # new\n",
    "    \"<|eot_id|>\"                                     # new\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  # new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here are three of the most famous and meaningful quotes from \"Charlotte's Web\":\n",
      "\n",
      "1. **\"Alone we can do so little; together we can do so much.\"** - This quote highlights the importance of friendship and teamwork, which is a central theme in the story of Charlotte and Wilbur.\n",
      "2. **\"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"** - This quote showcases the special bond between Charlotte and Wilbur, and also highlights the author's own appreciation for Charlotte's unique qualities.\n",
      "3. **\"What's the most important thing you've learned, Wilbur?\" \"I've learned that you are braver than you believe, and stronger than you seem, and smarter than you think.\"** - This quote is a heartwarming message from Charlotte to Wilbur, emphasizing the importance of self-confidence and believing in oneself.\n",
      "\n",
      "These quotes capture the essence of the story and its themes of friendship, loyalty, and the power of believing in oneself.\n"
     ]
    }
   ],
   "source": [
    "follow_up_response = llama31(follow_up_prompt)\n",
    "print(follow_up_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here are three of the most famous and meaningful quotes from \"Charlotte's Web\":\n",
       "\n",
       "1. **\"Alone we can do so little; together we can do so much.\"** - This quote highlights the importance of friendship and teamwork, which is a central theme in the story of Charlotte and Wilbur.\n",
       "2. **\"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"** - This quote showcases the special bond between Charlotte and Wilbur, and also highlights the author's own appreciation for Charlotte's unique qualities.\n",
       "3. **\"What's the most important thing you've learned, Wilbur?\" \"I've learned that you are braver than you believe, and stronger than you seem, and smarter than you think.\"** - This quote is a heartwarming message from Charlotte to Wilbur, emphasizing the importance of self-confidence and believing in oneself.\n",
       "\n",
       "These quotes capture the essence of the story and its themes of friendship, loyalty, and the power of believing in oneself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(follow_up_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here are three best quotes:\n",
       "\n",
       "1. **\"Believe you can and you're halfway there.\"** - Theodore Roosevelt\n",
       "\n",
       "This quote emphasizes the importance of having a positive mindset and believing in oneself. It suggests that having confidence and faith in one's abilities can lead to success.\n",
       "\n",
       "2. **\"The only way to do great work is to love what you do.\"** - Steve Jobs\n",
       "\n",
       "This quote highlights the significance of passion and enjoyment in one's work. It suggests that when we love what we do, we are more likely to be motivated, inspired, and produce high-quality work.\n",
       "\n",
       "3. **\"Success is not final, failure is not fatal: It is the courage to continue that counts.\"** - Winston Churchill\n",
       "\n",
       "This quote emphasizes the importance of perseverance and resilience in the face of challenges. It suggests that success is not a permanent state, and that failure is not a defining moment, but rather it's the courage to keep going that truly matters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Three Best quotes\"\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"  \n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    f\"{question}\"\n",
    "    \"<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "response = llama31(prompt,8)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"system\" role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "* \"Champions are made from something they have deep inside them - a desire, a dream, a vision.\" - Muhammad Ali\n",
       "* \"It's not whether you get knocked down, it's whether you get up.\" - Vince Lombardi\n",
       "* \"Hard work, dedication, and a willingness to learn are the keys to success in any field, including sports.\" - Michael Jordan"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Three Great quotes\"\n",
    "system_message = (\n",
    "    \"You are an expert in quotes about sports \" \n",
    "    \"You provide just the quotes and no commentary. \"\n",
    "    \"Reply in markdown\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"<|begin_of_text|>\"\n",
    "    \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    f\"{system_message}\"\n",
    "    \"<|eot_id|>\"    \n",
    "    \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    f\"{question}\"\n",
    "    \"<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "response = llama31(prompt,70)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "* \"The only way to get started is to quit talking and begin doing.\" - John Wooden\n",
       "* \"It's not about being the best, it's about being better than you were yesterday.\" - Bo Jackson\n",
       "* \"Success is not final, failure is not fatal: It is the courage to continue that counts.\" - Winston Churchill"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "follow_up_question = \"three more\"\n",
    "follow_up_prompt = (\n",
    "    \"<|begin_of_text|>\"\n",
    "    \"<|start_header_id|>system<|end_header_id|>\"      # system\n",
    "    f\"{system_message}\"                               # system\n",
    "    \"<|eot_id|>\"                                      # system\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"        # past\n",
    "    f\"{question}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"   # past\n",
    "    f\"{response}\"                                     # past\n",
    "    \"<|eot_id|>\"                                      # past\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"         # new\n",
    "    f\"{follow_up_question}\"                            # new     \n",
    "    \"<|eot_id|>\"                                       # new\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"    # new\n",
    ")\n",
    "response = llama31(follow_up_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Llama using higher-level message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|begin_of_text|>\"                              # start of prompt\n",
    "    \"<|start_header_id|>user<|end_header_id|>\"       # user header\n",
    "    \"Who wrote the book Charlotte's Web?\"                                    # user input\n",
    "    \"<|eot_id|>\"                                     #end of turn\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\"  #assistant header\n",
    ")\n",
    "response = llama31(prompt,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"Charlotte's Web\" was written by E.B. White.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Who wrote the book Charlotte's Web?\"\n",
    "    }\n",
    "  ]\n",
    "response = llama31(messages,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three notable quotes from \"Charlotte's Web\":\n",
      "\n",
      "1. \"Alone, alone, alone. That's the way it is when you're a pig. You're born, you live, you die. You're a meal in the making from the moment you're born.\"\n",
      "2. \"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"\n",
      "3. \"What's the most important thing in life? Answers will vary. But the most important thing in life is friendship.\"\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Three Best quotes\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an terse expert in children's literature\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Who wrote the book Charlotte's Web?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": response\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": follow_up_question\n",
    "    }\n",
    "  ]\n",
    "\n",
    "response = llama31(messages,8)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project usse case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.messages = []\n",
    "        if system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    def generate(self, user_question, model=8, temp=0):\n",
    "        self.messages.append({\"role\": \"user\", \"content\":user_question})\n",
    "        response = llama31(self.messages, model, temperature=temp)\n",
    "        self.messages.append({\"role\":\"assistant\", \"content\":response})\n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E.B. White wrote the book \"Charlotte\\'s Web\".'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a terse expert in Childrens literature.\" \n",
    "conv = Conversation(system_message)\n",
    "conv.generate(\"Who wrote the book Charlotte's Web?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are three quotes from \"Charlotte's Web\":\n",
       "\n",
       "1. \"Alone, no philosopher is a match for a flock of turkeys.\"\n",
       "2. \"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"\n",
       "3. \"Wherever you go, go with all your heart.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = conv.generate(\"What are three quotes\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a terse expert in Childrens literature.'},\n",
       " {'role': 'user', 'content': \"Who wrote the book Charlotte's Web?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'E.B. White wrote the book \"Charlotte\\'s Web\".'},\n",
       " {'role': 'user', 'content': 'What are three quotes'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Here are three quotes from \"Charlotte\\'s Web\":\\n\\n1. \"Alone, no philosopher is a match for a flock of turkeys.\"\\n2. \"It is not often that someone comes along who is a true friend and a good writer. Charlotte was both.\"\\n3. \"Wherever you go, go with all your heart.\"'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-lingual Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are 3 basic phrases in 8 languages:\n",
       "\n",
       "**English**\n",
       "1. Hello - Hello\n",
       "2. Thank you - Thank you\n",
       "3. Goodbye - Goodbye\n",
       "\n",
       "**German**\n",
       "1. Hallo - Hello\n",
       "2. Danke - Thank you\n",
       "3. Auf Wiedersehen - Goodbye\n",
       "\n",
       "**French**\n",
       "1. Bonjour - Hello\n",
       "2. Merci - Thank you\n",
       "3. Au revoir - Goodbye\n",
       "\n",
       "**Italian**\n",
       "1. Ciao - Hello/Goodbye\n",
       "2. Grazie - Thank you\n",
       "3. Arrivederci - Goodbye\n",
       "\n",
       "**Portuguese**\n",
       "1. Olá - Hello\n",
       "2. Obrigado/Obrigada - Thank you (male/female)\n",
       "3. Tchau - Goodbye\n",
       "\n",
       "**Hindi**\n",
       "1. नमस्ते (Namaste) - Hello\n",
       "2. धन्यवाद (Dhanyavad) - Thank you\n",
       "3. अलविदा (Alvida) - Goodbye\n",
       "\n",
       "**Spanish**\n",
       "1. Hola - Hello\n",
       "2. Gracias - Thank you\n",
       "3. Adiós - Goodbye\n",
       "\n",
       "**Thai**\n",
       "1. สวัสดี (Sawatdee) - Hello\n",
       "2. ขอขอบคุณ (Khop Khun Krap/Khob Kun Kha) - Thank you (male/female)\n",
       "3. สวัสดีครับ/สวัสดีค่ะ (Sawatdee Krap/Sawatdee Ka) - Goodbye (male/female)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_message = (\n",
    "    \"Answer in 8 languages: English, German, French, Italian,\" \n",
    "    \"Portuguese, Hindi, Spanish, and Thai\"\n",
    ")\n",
    "ml_conv = Conversation(system_message)\n",
    "response = ml_conv.generate(\"3 basic phrases\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python script for a Gradio chatbot app that you can run within a Jupyter notebook:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "# Initialize the Conversation class with a system message\n",
      "conversation = Conversation(\"You are an expert in literature. You provide brief replies.\")\n",
      "\n",
      "# Define a function to handle user input\n",
      "def chatbot(user_input, model_size, temperature):\n",
      "    response = conversation.generate(user_input, model=model_size, temp=temperature)\n",
      "    return response\n",
      "\n",
      "# Create a Gradio interface\n",
      "demo = gr.Interface(\n",
      "    fn=chatbot,\n",
      "    inputs=[\n",
      "        gr.Textbox(label=\"Ask a question\"),\n",
      "        gr.Radio(label=\"Model size\", choices=[8, 70, 405], value=8),\n",
      "        gr.Slider(label=\"Temperature\", minimum=0, maximum=1, step=0.1, value=0)\n",
      "    ],\n",
      "    outputs=gr.Textbox(label=\"Response\"),\n",
      "    title=\"Literature Expert Chatbot\",\n",
      "    description=\"Ask a question and get a brief response from a literature expert.\"\n",
      ")\n",
      "\n",
      "# Launch the Gradio app\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "This script initializes the `Conversation` class with a system message, defines a function to handle user input, creates a Gradio interface with input fields for user question, model size, and temperature, and launches the Gradio app. You can run this script within a Jupyter notebook to interact with the chatbot.\n",
      "\n",
      "Note: Make sure you have the `gradio` library installed in your Jupyter notebook environment. You can install it using `!pip install gradio` or `!conda install -c conda-forge gradio`.\n"
     ]
    }
   ],
   "source": [
    "code_query = \"\"\"\n",
    "I need a Python script for a Gradio chatbot app that should be run\n",
    "within a Jupyter notebook.\n",
    "1) When calling the LLM, use this class, which is already defined,\n",
    "    dont redefine it:\n",
    "class Conversation:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.messages = []\n",
    "        if system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def generate(self, user_question, model=8, temp=0):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_question})\n",
    "        response = llama31(self.messages, model, temperature=temp)  \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "2) initialize the class with a system message of:\n",
    "    \"You are an expert in liturature. You provide brief replies\"\n",
    "3) the llama() function is defined like this:\n",
    "    def llama(prompt_or_messages, model_size=8, \n",
    "              temperature=0, raw=False, debug=False):\n",
    "    and returns a reponse in a string. Don't redefine this.\n",
    "    valid model sizes are 8, 70 and 405.\n",
    "\"\"\"\n",
    "coder_system_message = (\n",
    "    \"You are an expert writing python Gradio chatbots\" \n",
    ")\n",
    "coder_conv = Conversation(coder_system_message)\n",
    "response = coder_conv.generate(code_query, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-41 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-41 (run)'.\n",
      "Exception in thread Thread-42 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-42 (run)'.\n",
      "Exception in thread Thread-43 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-43 (run)'.\n",
      "Exception in thread Thread-44 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-44 (run)'.\n",
      "Exception in thread Thread-45 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-45 (run)'.\n",
      "Exception in thread Thread-46 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-46 (run)'.\n",
      "Exception in thread Thread-47 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-47 (run)'.\n",
      "Exception in thread Thread-48 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "  File \"/opt/anaconda3/envs/dev_env/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-48 (run)'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m\n\u001b[1;32m     12\u001b[0m demo \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m     13\u001b[0m     fn\u001b[38;5;241m=\u001b[39mchatbot,\n\u001b[1;32m     14\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk a question and get a brief response from a literature expert.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/gradio/blocks.py:2368\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend, enable_monitoring)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2361\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2363\u001b[0m     (\n\u001b[1;32m   2364\u001b[0m         server_name,\n\u001b[1;32m   2365\u001b[0m         server_port,\n\u001b[1;32m   2366\u001b[0m         local_url,\n\u001b[1;32m   2367\u001b[0m         server,\n\u001b[0;32m-> 2368\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/gradio/http_server.py:149\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    140\u001b[0m         reloader \u001b[38;5;241m=\u001b[39m SourceFileReloader(\n\u001b[1;32m    141\u001b[0m             app\u001b[38;5;241m=\u001b[39mapp,\n\u001b[1;32m    142\u001b[0m             watch_dirs\u001b[38;5;241m=\u001b[39mGRADIO_WATCH_DIRS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             demo_file\u001b[38;5;241m=\u001b[39mGRADIO_WATCH_DEMO_PATH,\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     server \u001b[38;5;241m=\u001b[39m Server(config\u001b[38;5;241m=\u001b[39mconfig, reloader\u001b[38;5;241m=\u001b[39mreloader)\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, ServerFailedToStartError):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_env/lib/python3.10/site-packages/gradio/http_server.py:57\u001b[0m, in \u001b[0;36mServer.run_in_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarted:\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServerFailedToStartError(\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer failed to start. Please check that the port is available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Initialize the Conversation class with a system message\n",
    "conversation = Conversation(\"You are an expert in literature. You provide brief replies.\")\n",
    "\n",
    "# Define a function to handle user input\n",
    "def chatbot(user_input, model_size, temperature):\n",
    "    response = conversation.generate(user_input, model=model_size, temp=temperature)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=chatbot,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Ask a question\"),\n",
    "        gr.Radio(label=\"Model size\", choices=[8, 70, 405], value=8),\n",
    "        gr.Slider(label=\"Temperature\", minimum=0, maximum=1, step=0.1, value=0)\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    title=\"Literature Expert Chatbot\",\n",
    "    description=\"Ask a question and get a brief response from a literature expert.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The quote you're referring to is from Ernest Hemingway's novel \"The Old Man and the Sea\". The full quote is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they were the same color as the sea and they seemed to hold the moon on their dark blue waters.\"\n",
      "\n",
      "This quote is from the first chapter of the novel, and it sets the tone for the rest of the story, which is a poignant and introspective exploration of the old man's life, his struggles, and his connection to the natural world.\n"
     ]
    }
   ],
   "source": [
    "question = \"The quote that starts with 'Everything about him is old' in The Old Man and the Sea\"\n",
    "\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The quote you're thinking of is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they seemed the same color as the sea and were cheerful and undefeated.\"\n",
      "\n",
      "- Ernest Hemingway, The Old Man and the Sea\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You're thinking of the famous quote from Ernest Hemingway's \"The Old Man and the Sea\"!\n",
      "\n",
      "The full quote is:\n",
      "\n",
      "\"Everything about him was old except his eyes and they were the same color as the sea and were cheerful and undefeated.\"\n",
      "\n",
      "Isn't that a beautiful and poignant description of the old man, Santiago?\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
